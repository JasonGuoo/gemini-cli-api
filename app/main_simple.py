#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ Gemini CLI OpenAI å…¼å®¹ API
ä½¿ç”¨ -p å‚æ•°ç›´æ¥è°ƒç”¨ï¼Œæ— éœ€å¤æ‚çš„è¿›ç¨‹æ± ç®¡ç†
"""

from fastapi import FastAPI, HTTPException, status
from fastapi.responses import StreamingResponse
from app.models import ChatCompletionRequest, ChatCompletionResponse, ChatMessage, Choice, Usage
from app.services.gemini_simple import SimplifiedGeminiCLI
from app.config import GEMINI_CLI_PATH, DEBUG_DUMP_ENABLED, DEBUG_DUMP_DIR
import asyncio
import time
import json
import os
import datetime
import uuid

app = FastAPI(
    title="Gemini CLI OpenAI Compatible API (Simplified)",
    description="A simplified RESTful API that wraps the gemini-cli tool using -p parameter.",
    version="0.2.0",
)

# Global Gemini CLI instance
gemini_cli: SimplifiedGeminiCLI = None

async def dump_debug_info(filename: str, data: dict):
    """ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
    if not DEBUG_DUMP_ENABLED:
        return
    
    filepath = os.path.join(DEBUG_DUMP_DIR, filename)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2)

@app.on_event("startup")
async def startup_event():
    global gemini_cli
    print("ğŸš€ Starting Simplified Gemini CLI API server...")
    print(f"ğŸ”§ CLI path: {GEMINI_CLI_PATH}")
    print(f"ğŸ› Debug dumps: {'enabled' if DEBUG_DUMP_ENABLED else 'disabled'}")
    
    try:
        # åˆ›å»ºç®€åŒ–ç‰ˆ CLI å®ä¾‹
        gemini_cli = SimplifiedGeminiCLI(cli_path=GEMINI_CLI_PATH)
        
        # æµ‹è¯•è¿æ¥
        model_info = await gemini_cli.get_model_info()
        if model_info["available"]:
            print(f"âœ… Gemini CLI available! Default model: {model_info['default_model']}")
        else:
            print(f"âš ï¸ Gemini CLI test failed: {model_info.get('error', 'Unknown error')}")
        
        print("ğŸŒ Server ready to handle requests!")
        
    except Exception as e:
        print(f"âŒ Failed to initialize Gemini CLI: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"Failed to start application: {e}"
        )

@app.on_event("shutdown")
async def shutdown_event():
    print("ğŸ›‘ Shutting down Simplified Gemini CLI API server...")
    print("ğŸ‘‹ Server stopped.")

async def generate_stream_response(request: ChatCompletionRequest, debug_dump_data: dict):
    """ç”Ÿæˆæµå¼å“åº”"""
    request_id = debug_dump_data["request_id"]
    
    # åˆå¹¶æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯
    full_prompt = "\n".join([msg.content for msg in request.messages if msg.role == "user"])
    
    # è·å–æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    model = "gemini-2.5-pro" if request.model == "gemini-cli" else request.model
    
    print(f"ğŸ”„ Executing streaming prompt for request {request_id}")
    
    try:
        # æµå¼æ‰§è¡Œ
        async for chunk in gemini_cli.execute_prompt_stream(full_prompt, model=model, timeout=120):
            if chunk.strip():
                # åˆ›å»ºå“åº”æ¶ˆæ¯
                response_message = ChatMessage(role="assistant", content=chunk)
                choice = Choice(index=0, message=response_message, finish_reason=None)
                response = ChatCompletionResponse(
                    id=f"chatcmpl-{request_id}",
                    created=int(time.time()),
                    model=request.model,
                    choices=[choice],
                    usage=None  # æµå¼å“åº”ä¸­ä¸åŒ…å«usage
                )
                
                yield f"data: {json.dumps(response.dict(exclude_unset=True))}\n\n"
                
                # è®°å½•è°ƒè¯•ä¿¡æ¯
                debug_dump_data["cli_interactions"].append({
                    "type": "stream_chunk",
                    "content": chunk,
                    "timestamp": datetime.datetime.now().isoformat()
                })
        
        # å‘é€æœ€ç»ˆå“åº”
        final_usage = Usage(
            prompt_tokens=gemini_cli.estimate_tokens(full_prompt),
            completion_tokens=0,  # æµå¼æ¨¡å¼ä¸‹éš¾ä»¥å‡†ç¡®è®¡ç®—
            total_tokens=0
        )
        
        final_choice = Choice(
            index=0, 
            message=ChatMessage(role="assistant", content=""), 
            finish_reason="stop"
        )
        
        final_response = ChatCompletionResponse(
            id=f"chatcmpl-{request_id}",
            created=int(time.time()),
            model=request.model,
            choices=[final_choice],
            usage=final_usage
        )
        
        yield f"data: {json.dumps(final_response.dict(exclude_unset=True))}\n\n"
        yield "data: [DONE]\n\n"
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        debug_dump_data["final_response"] = final_response.dict(exclude_unset=True)
        await dump_debug_info(f"{request_id}_stream.json", debug_dump_data)
        
    except Exception as e:
        print(f"âŒ Stream error: {e}")
        error_response = {
            "error": {
                "message": str(e),
                "type": "internal_error",
                "code": "gemini_cli_error"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
        yield "data: [DONE]\n\n"
        
        debug_dump_data["error"] = str(e)
        await dump_debug_info(f"{request_id}_error.json", debug_dump_data)

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚"""
    request_id = str(uuid.uuid4())
    debug_dump_data = {
        "request_id": request_id,
        "timestamp_start": datetime.datetime.now().isoformat(),
        "request_body": request.dict(),
        "cli_interactions": []
    }
    
    print(f"ğŸ“¨ Processing request {request_id}, stream: {request.stream}")
    
    try:
        # æ£€æŸ¥ Gemini CLI æ˜¯å¦å¯ç”¨
        if not gemini_cli:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Gemini CLI service not available"
            )
        
        # åˆå¹¶æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯
        full_prompt = "\n".join([msg.content for msg in request.messages if msg.role == "user"])
        
        if not full_prompt.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No user message found in request"
            )
        
        # è·å–æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        model = "gemini-2.5-pro" if request.model == "gemini-cli" else request.model
        
        # æµå¼å“åº”
        if request.stream:
            return StreamingResponse(
                generate_stream_response(request, debug_dump_data),
                media_type="text/event-stream"
            )
        
        # éæµå¼å“åº”
        print(f"ğŸ”„ Executing non-streaming prompt for request {request_id}")
        
        start_time = time.time()
        response_content = await gemini_cli.execute_prompt(
            full_prompt, 
            model=model, 
            timeout=120
        )
        execution_time = time.time() - start_time
        
        print(f"âœ… Got response in {execution_time:.2f}s, length: {len(response_content)} chars")
        
        # è®°å½•è°ƒè¯•ä¿¡æ¯
        debug_dump_data["cli_interactions"].append({
            "type": "complete_response",
            "prompt": full_prompt,
            "response": response_content,
            "execution_time": execution_time,
            "timestamp": datetime.datetime.now().isoformat()
        })
        
        # ä¼°ç®—tokenä½¿ç”¨é‡
        prompt_tokens = gemini_cli.estimate_tokens(full_prompt)
        completion_tokens = gemini_cli.estimate_tokens(response_content)
        total_tokens = prompt_tokens + completion_tokens
        
        # åˆ›å»ºå“åº”
        response_message = ChatMessage(role="assistant", content=response_content)
        choice = Choice(index=0, message=response_message, finish_reason="stop")
        usage = Usage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
        
        response = ChatCompletionResponse(
            id=f"chatcmpl-{request_id}",
            created=int(time.time()),
            model=request.model,
            choices=[choice],
            usage=usage
        )
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        debug_dump_data["final_response"] = response.dict()
        await dump_debug_info(f"{request_id}_success.json", debug_dump_data)
        
        return response
        
    except Exception as e:
        print(f"âŒ Request {request_id} failed: {e}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        debug_dump_data["error"] = str(e)
        debug_dump_data["timestamp_error"] = datetime.datetime.now().isoformat()
        await dump_debug_info(f"{request_id}_error.json", debug_dump_data)
        
        # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›é€‚å½“çš„HTTPçŠ¶æ€ç 
        if isinstance(e, TimeoutError):
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail=f"Request timed out: {str(e)}"
            )
        elif "not found" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Gemini CLI not available: {str(e)}"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Internal error: {str(e)}"
            )

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        model_info = await gemini_cli.get_model_info()
        
        models = [
            {
                "id": "gemini-cli",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "google",
            },
            {
                "id": model_info["default_model"],
                "object": "model", 
                "created": int(time.time()),
                "owned_by": "google",
            }
        ]
        
        return {"object": "list", "data": models}
        
    except Exception as e:
        print(f"âŒ Failed to get model info: {e}")
        # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
        return {
            "object": "list", 
            "data": [
                {
                    "id": "gemini-cli",
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "google",
                }
            ]
        }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        model_info = await gemini_cli.get_model_info()
        return {
            "status": "healthy" if model_info["available"] else "degraded",
            "gemini_cli": model_info,
            "timestamp": datetime.datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.datetime.now().isoformat()
        }

# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 